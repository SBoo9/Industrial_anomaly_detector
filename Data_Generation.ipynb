{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fedb0f9-f1cb-4025-8d0b-c98ecc571fd3",
   "metadata": {},
   "source": [
    "# Industrial Failure Detection System Data Generation\n",
    "\n",
    "In this notebook, we generate synthetic sensor data for an industrial failure detection system. The generated dataset simulates the behavior of multiple sensors under normal operating conditions, with some anomalies introduced to mimic failure events.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Necessary Libraries\n",
    "\n",
    "We begin by importing the required libraries: `numpy` and `pandas`. These are used for numerical computations and data manipulation, respectively.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf57fe5-bf74-4e2d-b76b-6dc0cb74558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13525881-ee48-4929-b561-78373427e763",
   "metadata": {},
   "source": [
    "## Step 2: Define Helper Functions\n",
    "generate_normal_sensor_data\n",
    "This function generates synthetic sensor data under normal operating conditions. The data simulates periodic fluctuations, such as those caused by cyclic processes in machines, with added random noise.\n",
    "\n",
    "Parameters:\n",
    "sensor_name: Name of the sensor.\n",
    "num_points: Number of data points to generate.\n",
    "min_val and max_val: Range of sensor values.\n",
    "noise_factor: Degree of random noise to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6706f9-ce8f-43db-a6ce-7db3409be4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_sensor_data(sensor_name, num_points, min_val, max_val, noise_factor=0.1):\n",
    "    \"\"\"Generates normal operating sensor data with periodic fluctuations.\"\"\"\n",
    "    time = np.arange(0, num_points)\n",
    "    # Simulate cyclic behavior (like sinusoidal pattern) with some random noise\n",
    "    data = (np.sin(time / 50) + 1) * (max_val - min_val) / 2 + min_val\n",
    "    noise = np.random.normal(0, (max_val - min_val) * noise_factor, num_points)\n",
    "    return data + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f173b-14fb-49bc-8963-eb31a06ad18e",
   "metadata": {},
   "source": [
    "introduce_failures\n",
    "This function introduces anomalies into the dataset to simulate failure events. These anomalies manifest as significant changes in sensor readings, such as spikes in temperature or vibration.\n",
    "\n",
    "Parameters:\n",
    "df: Input DataFrame containing sensor data.\n",
    "num_failures: Number of failure events to introduce.\n",
    "failure_duration: Duration (in data points) of each failure event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4bb427-b191-46c9-a833-d3642c96161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_failures(df, num_failures, failure_duration=60):\n",
    "    \"\"\"Introduce anomalies just before failure events.\"\"\"\n",
    "    max_start_index = len(df) - failure_duration  # Ensure we don't go out of bounds\n",
    "    failure_indices = np.random.choice(max_start_index, num_failures, replace=False)  # Prevent exceeding DataFrame length\n",
    "    \n",
    "    for idx in failure_indices:\n",
    "        # Adjust the end index to ensure it doesn't exceed the DataFrame length\n",
    "        end_idx = idx + failure_duration\n",
    "        if end_idx > len(df):\n",
    "            end_idx = len(df)  # Prevent out-of-bound errors\n",
    "        \n",
    "        # Add abnormal changes in temperature, vibration, etc.\n",
    "        df.loc[idx:end_idx-1, 'temperature'] += np.random.randint(20, 50, end_idx - idx)\n",
    "        df.loc[idx:end_idx-1, 'vibration'] += np.random.uniform(3, 5, end_idx - idx)\n",
    "        df.loc[idx:end_idx-1, 'failure'] = 1  # Failure flag for the duration of the anomaly\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e55993-d555-412b-a045-b626a1c1a483",
   "metadata": {},
   "source": [
    "## Step 3: Generate Sensor Data\n",
    "Normal Operating Data\n",
    "We generate synthetic sensor data for temperature, vibration, pressure, and humidity under normal conditions. Each sensor's data exhibits periodic fluctuations with added noise.\n",
    "\n",
    "Parameters:\n",
    "num_points: Number of data points (30 days of 1-minute intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71987393-526d-479c-8970-743e20c34338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sensor data for one device\n",
    "num_points = 43200  # 30 days of 1-minute intervals\n",
    "temperature_data = generate_normal_sensor_data('temperature', num_points, 20, 40)\n",
    "vibration_data = generate_normal_sensor_data('vibration', num_points, 0, 2)\n",
    "pressure_data = generate_normal_sensor_data('pressure', num_points, 100, 200)\n",
    "humidity_data = generate_normal_sensor_data('humidity', num_points, 30, 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12875ee-ae10-4829-9a96-5051db776791",
   "metadata": {},
   "source": [
    "Create Initial DataFrame\n",
    "We combine the generated sensor data into a pandas DataFrame and initialize a failure flag (failure = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063388c9-ec92-4559-8cf5-566a10bedda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': pd.date_range(start='2024-12-01', periods=num_points, freq='1min'),\n",
    "    'temperature': temperature_data,\n",
    "    'vibration': vibration_data,\n",
    "    'pressure': pressure_data,\n",
    "    'humidity': humidity_data,\n",
    "    'failure': 0  # Default no failure\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b780aa-cffa-4407-856e-cb56791ddcf0",
   "metadata": {},
   "source": [
    "## Step 4: Introduce Failures\n",
    "We introduce 5 failure events, each lasting for 60 minutes. During these events, temperature and vibration exhibit abnormal spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0712e6-2a9b-4335-b547-f9ac7a724066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce failures\n",
    "df = introduce_failures(df, num_failures=5, failure_duration=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a3747-9451-49fb-92a2-d712944d711d",
   "metadata": {},
   "source": [
    "## Step 5: Simulate Multiple Devices\n",
    "We simulate data for 10 different machines by duplicating the initial DataFrame and assigning a unique machine ID to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d618be3-dccb-4dcc-a924-fc7c9802f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = []\n",
    "for device_id in range(1, 11):  # 10 devices\n",
    "    df_device = df.copy()\n",
    "    df_device['machine_id'] = device_id\n",
    "    devices.append(df_device)\n",
    "\n",
    "df_full = pd.concat(devices, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bcd0b-fabc-4497-81ca-4932f8c107a4",
   "metadata": {},
   "source": [
    "## Step 6 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892782c9-4374-46a5-bbdb-2395a093533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('synthetic_iot_sensor_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ebfbc-886d-4a9b-a420-5cdaaa175ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "08649fce-5c7f-421b-90c2-d20a6d749b59",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "def load_data(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return None\n",
    "\n",
    "# Feature engineering (scaling, normalization, etc.)\n",
    "def feature_engineering(df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df.drop(columns=['timestamp', 'machine_id'], errors='ignore'))\n",
    "    return X_scaled\n",
    "\n",
    "# Train Isolation Forest and detect anomalies\n",
    "def train_isolation_forest(X_scaled):\n",
    "    # Hyperparameters\n",
    "    n_estimators = 500\n",
    "    contamination = 0.05\n",
    "    \n",
    "    # Train model\n",
    "    clf = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42)\n",
    "    clf.fit(X_scaled)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    anomaly_predictions = clf.predict(X_scaled)\n",
    "    anomaly_predictions = np.where(anomaly_predictions == -1, 1, 0)  # 1: anomaly, 0: normal\n",
    "    \n",
    "    return clf, anomaly_predictions\n",
    "\n",
    "# PCA visualization\n",
    "def plot_pca(X_scaled, anomaly_predictions):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=anomaly_predictions, cmap='coolwarm', edgecolor='k', s=20)\n",
    "    plt.title(\"PCA Visualization of Anomalies\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.colorbar(label=\"Anomaly (1) or Normal (0)\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyze anomalies\n",
    "def analyze_anomalies(df, anomaly_predictions):\n",
    "    df['anomaly'] = anomaly_predictions\n",
    "    anomaly_count = df['anomaly'].sum()\n",
    "    print(f\"Total anomalies detected: {anomaly_count}\")\n",
    "    \n",
    "    # Display a few flagged anomalies for manual inspection\n",
    "    anomalies = df[df['anomaly'] == 1]\n",
    "    print(\"Sample anomalies:\")\n",
    "    print(anomalies.head())\n",
    "\n",
    "# Main execution flow\n",
    "def main():\n",
    "    df = load_data('processed_iot_sensor_data.csv')\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    X_scaled = feature_engineering(df)\n",
    "    \n",
    "    # Train IsolationForest\n",
    "    clf, anomaly_predictions = train_isolation_forest(X_scaled)\n",
    "    \n",
    "    # Analyze and visualize anomalies\n",
    "    analyze_anomalies(df, anomaly_predictions)\n",
    "    plot_pca(X_scaled, anomaly_predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbfcaa-88d1-499a-a353-619243ca6a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
